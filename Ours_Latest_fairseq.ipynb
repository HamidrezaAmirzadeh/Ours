{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYWUf7xxZ1yP"
      },
      "outputs": [],
      "source": [
        "!git clone -b kob_2020 https://github.com/HamidrezaAmirzadeh/Ours.git\n",
        "# !git clone https://github.com/HamidrezaAmirzadeh/Ours.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mosestokenizer -q\n",
        "!pip install sacremoses -q\n",
        "!pip install fastBPE -q"
      ],
      "metadata": {
        "id": "wvzCuU18eJRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Before running this code, make sure of your modifications to the fairseq, because\n",
        "I couldn't manage to get importlib.reload to work and reload the installed fairseq\n",
        "\"\"\"\n",
        "%cd Ours/fairseq_latest/\n",
        "!pip install -e."
      ],
      "metadata": {
        "id": "10F1nbbV1mcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# import importlib\n",
        "# importlib.reload(fairseq.models.transformer)\n",
        "from fairseq.models.transformer import TransformerModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "qBTH1U18deeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download published trained model (2.79GB)\n",
        "!wget \"https://dl.fbaipublicfiles.com/fairseq/models/wmt19.de-en.joined-dict.single_model.tar.gz\""
      ],
      "metadata": {
        "id": "MmkAuQOWc26u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model\n",
        "\n",
        "path = './wmt19.de-en.joined-dict.single_model.tar.gz'\n",
        "\n",
        "de2en = TransformerModel.from_pretrained(\n",
        "    model_name_or_path=path,\n",
        "    tokenizer='moses',\n",
        "    bpe='fastbpe',\n",
        ").to(device)\n",
        "\n",
        "de2en.eval()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dNzsdHJdhs-",
        "outputId": "3656ae74-1117-47ca-8a4a-d9e4def010d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "de = \"Wir glauben nicht, daß wir nur Rosinen herauspicken sollten.\"\n",
        "en = \"We do not believe that we should cherry-pick.\"\n",
        "\n",
        "eos_id = de2en.tgt_dict.index(\"</s>\")\n",
        "\n",
        "# Get indices and tokenized text\n",
        "print(\"De (source)\")\n",
        "print(\"Raw text:\", de)\n",
        "de_idx = de2en.encode(de).unsqueeze(0).to(device)\n",
        "print(\"Indices:\", de_idx)\n",
        "de_toks = [de2en.src_dict[i] for i in de_idx[0]]\n",
        "print(\"Tokenized text:\", de_toks)\n",
        "\n",
        "print(\"\\nEn (target)\")\n",
        "print(\"Raw text:\", en)\n",
        "en_idx = torch.tensor([eos_id] + [de2en.tgt_dict.index(t) for t in de2en.bpe.encode(en).split()]).unsqueeze(0).to(device)\n",
        "print(\"Indices:\", en_idx)\n",
        "en_toks = [de2en.tgt_dict[i] for i in en_idx[0]]\n",
        "print(\"Tokenized text:\", en_toks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sGJsRuHdz7I",
        "outputId": "0729ebbf-151f-4533-ff8d-372b04b2a104"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "De (source)\n",
            "Raw text: Wir glauben nicht, daß wir nur Rosinen herauspicken sollten.\n",
            "Indices: tensor([[  396,  7952,    74,     4,   514,   150,   249,  2759,  4089,   730,\n",
            "           640,   393, 15810,  1882,     5,     2]])\n",
            "Tokenized text: ['Wir', 'glauben', 'nicht', ',', 'daß', 'wir', 'nur', 'Ros@@', 'inen', 'her@@', 'au@@', 'sp@@', 'icken', 'sollten', '.', '</s>']\n",
            "\n",
            "En (target)\n",
            "Raw text: We do not believe that we should cherry-pick.\n",
            "Indices: tensor([[    2,   333,   283,    90,  2989,    38,   136,   505,   199,  5939,\n",
            "           167,   406, 21075,     5]])\n",
            "Tokenized text: ['</s>', 'We', 'do', 'not', 'believe', 'that', 'we', 'should', 'ch@@', 'err@@', 'y@@', '-@@', 'pick@@', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = de2en.models[0](de_idx, de_idx.size()[-1], en_idx, output_all_attentions=True)"
      ],
      "metadata": {
        "id": "BJ0Fhc8rd2MO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the version of the libraries used in this environment\n",
        "!pip freeze > requirements.txt\n",
        "from google.colab import files\n",
        "files.download(\"requirements.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bcalQZOtjUF2",
        "outputId": "0daaa496-145f-45d6-f0ef-502efd7a9dcb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e50dab68-d678-47b2-9735-b31c3d2ab348\", \"requirements.txt\", 10851)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}